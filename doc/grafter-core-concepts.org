#+TITLE: Grafter Core Concepts & Semantics

These are some follow up notes to clarify the semantics of Grafter,
and help answer any questions you may have.

* Overview

It's quite a challenge trying to explain an API written in a foreign
programming language without making it appear too scary.  And I
appreciate some of the concepts, like immutability, laziness &
homoiconicity, higher order functions, macros, and dare I say it
monads are unfamiliar.

However users should never need to see the machinery, they just use
the higher level abstractions that the tool presents...  and these
should be simple and more familiar, and include:

- Chainable function pipelines that can be built through a
  drag-and-drop pallete with live preview and can be customised at
  runtime with a new context... e.g. different ways of handling
  errors.

- Table level transformations (e.g. merge columns 2 & 3, remove the
  first row, apply these functions to these columns)

- Graph pattern templates.

- (Maybe some other steps to customise validation, loading etc...)

* Pluggable function pipelines for cell converters

Functions are simply composed in a pipeline.  Users can select
functions from a palete and place them one after the other, to convert
values from cells into the appropriate rdf type.

One necessary limitation here, to prevent users from manually wiring
arguments together is that the functions in the pipeline can only take
one argument.  This is necessary for composition i.e the following is
expressible in a pipeline, and no intermediate variables are needed:

#+BEGIN_SRC clojure
(c (b (a "initial value))) ;; => "output value"
#+END_SRC

In ruby you would write this as:

#+BEGIN_SRC ruby
c(b(a("initial value")))
#+END_SRC

The only difference is that in clojure the function name occurs inside
the parentheses as the first argument.  Now the trick that the
pipelines do is that they essentially convert:

#+BEGIN_SRC clojure
(c (b (a "value")))
#+END_SRC

Into a function that can do the work later, rather than immediately
e.g:

#+BEGIN_SRC clojure
(fn [arg]
  (c (b (a arg)))
#+END_SRC

In clojure you don't even need to build functions by hand like this
you can use the function comp to do the same:

#+BEGIN_SRC clojure
(comp c b a) ;; => (fn [arg] (c (b (a arg)))
#+END_SRC

The lack of multiple arguments is necessary for composition, but it's
not really a problem because you can (and I do) build one argument
functions that close over arguments provided to their "factory
function".

Part of what I'm proposing is that we provide as components a suite of
factory functions that let users build and customise their own
functions for these pipelines.  You can see this already with the
following functions:

- prefixer - takes a string argument and returns a 1-argument function
  that concatenates its argument to the end of the configured prefix.

- mapper - Takes a hashmap (a lookup table in the UI) and returns a
  function that will replace supplied values.  If no substitution is
  performed, it passes the original value through unaltered.  (I use
  this to replace the empty string with 0 for attendances).

- replacer - Similar to mapper, but it takes a regex

An example of their use with comp looks like this... m-chain works
similarly.

#+BEGIN_SRC clojure
(comp parse-int
           (replacer "," "")
           trim
           (mapper {"" "0"}))
#+END_SRC

(I suspect mapper/replacer can be replaced by a better combination of
functions).

* (not so scary?) Monadic Pipelines

So why not just use composition?  Why use monads?  Well, I don't want
to have to explain monads (1000's try, few succeed), but I think
monads might be useful here...

So you'll remember the pipeline makes calls to m-chain e.g.

#+BEGIN_SRC clojure
(m-chain [trim replace-comma parse-int])
#+END_SRC

Conceptually this is almost the same as:

#+BEGIN_SRC clojure
(comp parse-int replace-comma trim)
#+END_SRC

Both return a function.  There are two main differences:

1) comp reads in the same direction as composition i.e. right to left,
   whilst m-chain reads left-right... no big deal... either could read
   either way.

2) m-chain does the composition but it supports what I like to call
   "programmable semi-colons"... i.e. it supports arbitrary logic
   between the links in the chain.

m-chain needs to run within the context of a monad which provides this
custom logic...  To get the pure composition logic we run it with the
identity monad identity-m.  The identity monad essentially does
nothing between the compositions.

#+BEGIN_SRC clojure
(with-monad identity-m
  (m-chain [trim replace-comma parse-int]))
#+END_SRC

There may be a problem with this code though, which is that parse-int raises exceptions if it's given anything it can't parse:

#+BEGIN_SRC clojure
(def parse-attendance (with-monad identity-m
                                       (m-chain [trim replace-comma parse-int])))

(parse-attendance "1,000  ") ;; => 1000
#+END_SRC

In particular if it receives an empty string, it will raise an exception.

#+BEGIN_SRC clojure
(parse-attendance "") ;; => EXCEPTION ...
#+END_SRC

To fix this we can run it in grafters blank-m monad:

#+BEGIN_SRC clojure
(def parse-attendance (with-monad blank-m
                                       (m-chain [trim replace-comma parse-int])))

(parse-attendance "") ;; => ""
#+END_SRC

blank-m, treats nil or "" as a pipeline failure, where either nil or
an empty string will abort the pipeline and return an empty string:
"".  By swapping the monad we can control the behaviour of the
pipeline.  Without the monad, we'd have to modify every function
themselves with the appropriate logic.

In order for standard functions to be used within a monad, they need
to be lifted into it with lift-1.  Users would never or need to do
this, it can be done for them.  They'd simply select the function they
want, and the type of pipeline (monad).  Additionally if we want to
capture different log outputs etc... we should be able to do so by
modifying the monad.

I suspect that this pipeline DSL, when mapped to columns can also be
used to detect and warn about changes in the source spreadsheet; as
essentially these functions are descriptions of what types are
expected in what columns.  Table Processing DSL In terms of DSL code
presented yesterday I suspect that this is the best understood part of
Grafter, and needs a little less explanation in terms of semantics.
Though the laziness and mechanics of how and when processing is done
are perhaps less clear; they're probably better served by an
introduction to Clojure; once this is understood the code is really
very simple.  None of the magic here is mine, it was all given to me
be Clojure.

The two key ideas behind this bit of the DSL are:

1) It is where users get to wire together the bulk of the data
   management operations.  If they need to use multi argument
   functions they can do so by applying an arbitrary function from the
   palette to selected columns with fuse.

2) The bigger idea behind this bit of the DSL, is really that it's
   where users get to wire much of the data together, with a tool that
   always offers live feedback... in a manner similar to what [[http://worrydream.com/#!/LearnableProgramming][Bret
   Victor proposes]].

I can imagine two different styles of interface that could be directly
built on this DSL...  One would be a traditional Kettle-like workflow
tool.  The other a more simple Refine-like tool.

Ultimately I suspect you'd choose a Kettle-like tool if you wanted to
support conditionals within the Table Processing DSL and a refine-like
one if you didn't.

Personally I'm eering against supporting conditionals in the table
processor.  Conditionals feel like they demand a significant amount of
additional complexity in UI and infrastructure, and I suspect we can
build more specialised forms of condition, that are more appropriate
for the task at hand, and easier for end users.  (For example I'm
considering adding optionals to Grafters RDF DSL, which would work
similarly to a construct, with a partial match.)

One might also argue that any Kettle workflow with conditionals in it
would be better written in code....  I suspect most are quite linear;
maybe 70% of all workflows don't need conditionals, and having them
takes you to 80%, after which you need code anyway?  This is all just
speculation... but it's a hunch that's informing my preference to
avoid them for the time being, as conditionals will bring a huge
amount of baggage and imply recursive hierarchies.  So lets assume a
refine like interface for now:

Imagine a spreadsheet like interface, displaying the first 50 rows of
the loaded CSV file... Right now their computation is:

#+BEGIN_SRC clojure
'(-> (parse-csv "./data/attendances.csv"))
#+END_SRC

A pane on the interface shows loading the specific CSV as the first
"commit" (operation) in the operations timeline.

#+BEGIN_SRC
- File "./data/attendances.csv"  <--- SELECTED
#+END_SRC

They select the first row, right click and click drop...  The drop
menu item, is bound not to the underlying drop function, but to a
function which knows how to create a call to that function,
parameterised by the selection, in this case 1 is the argument, so the
function creates a new computation and by adding (drop 1) to the old
computation:

#+BEGIN_SRC clojure
'(-> (parse-csv "./data/attendances.csv")
(drop 1))
#+END_SRC

Note that the quote prefixing the list means that this is clojure
data... it just so happens that what the data also represents happens
to be clojure program.  This means, we can wrap the program in
whatever context we want and evaluate it, and let the user inspect and
manipulate it further.

The UI shows the following operations (time flows up on this
representation and down in the code):

#+BEGIN_SRC
- Drop Row 1          <--- SELECTED
- File "./data/attendances.csv"
#+END_SRC

The user can roll back time and view the results of earlier operations
by clicking back and forth on the operations.  This should effectively
make debugging very easy, and is an idea we can borrow from Open
Refine.

Errors in the preview mode (monad?) should display on the appropriate
CSV column / row / cell.

* RDFizing DSL

The job of the table DSL is ultimately to convert a lazy-sequence of
rows containing Strings into a lazy-sequence of rows containing RDF
types.  These RDF types are then passed into the RDFizing DSL.

The RDF types here, are primarily Clojure and Java types which are
currently.

- java Integers and numerical types
- java.lang.String (considered to be URI's)
- java.net.URL (considered to be URI's)
- java.net.URI (considered to be a URI)
- java.util.Date (considered to be an xsd::dateTime)
- Anything that can be converted to an RDF type... i.e. anything that implements the grafter ISesameRDFConverter protocol.  This protocol is already extended to many sesame types, but also crucially allows (s "string") and (s "my string :en) to be used to create literal RDF strings.
- clojure.lang.Keywords (considered to be identifiers for blank nodes)

Any of these can be used to make an intermediate type either a
=grafter.rdf.protocols.Triple= a =grafter.rdf.protocols.Quad= (or
anything that implements the =grafter.rdf.protocols/IStatement=
protocol).

We define a graph function that takes a graph uri string and an rdf
graph in turtle-like syntax:

#+BEGIN_SRC clojure
(graph "http://mygraph.com/graph/test"
    [subject [[rdf:a "Person"]
                  [rdfs:label "John Doe"]
                  [vcard:hasAddress [[rdf:a vcard:Address]
                                                  [vcard:streetAddress street-address]]]])
#+END_SRC

Note the graph function coverts my Turtle like DSL syntax into a lazy-sequence of abstract RDF statements.  e.g. the above expression yields a lazy-sequence of Quads:

#+BEGIN_SRC clojure
;; => (#grafter.rdf.protocols.Quad{:s "http://john-doe.com/id/johnd", :p "http://www.w3.org/1999/02/22-rdf-syntax-ns#type", :o "http://foaf.com/Person", :c "http://mygraph.com/graph/test"} #grafter.rdf.protocols.Quad{:s "http://john-doe.com/id/johnd", :p "http://www.w3.org/2000/01/rdf-schema#label", :o #<rdf$s$reify__5355 John Doe>, :c "http://mygraph.com/graph/test"})
#+END_SRC

The graphify macro, is the only macro I've written in grafter so far,
and it's essentially a specialised anonymous function that compiles
something like this:

#+BEGIN_SRC clojure
(graphify [a b c]
  (graph "http://foobar.com/"
     ; ...)

  (graph "http://foobarbaz.com/"
     ; ...))
#+END_SRC

Into something like this:

#+BEGIN_SRC clojure
(fn ([row]
       (->> row
            (mapcat
             (fn [[a b c :as row9652]]
               (->>
                (concat
                 (graph "http://foobar.com/"
                        ;; triples...
                        )
                 (graph "http://foobarbaz.com/"
                        ;; triples...
                        ))
                (map (fn [triple] (with-meta triple {:row row9652})))))))))
#+END_SRC

The main job it does, is concatenate all the sequences each graph
clause returns into one big flat sequence.

This ensure that the row is currently reported alongside the triple if
an exception is raised later in the pipeline.

It also attaches a piece of hidden meta-data to each triple, which is
the row it came from.  Clojure meta-data never affects the value (or
equality) of an object, but it is copied along with it.

This flat lazy-sequence of immutable RDF statements are what is
finally passed into the importer, which does the final checking, type
conversion, and loading of the RDF into the triple store (though more
stages could be added should we wish).

#+BEGIN_SRC clojure
;; => (#grafter.rdf.protocols.Quad{:s "http://john-doe.com/id/johnd", :p "http://www.w3.org/1999/02/22-rdf-syntax-ns#type", :o "http://foaf.com/Person", :c "http://mygraph.com/graph/test"} #grafter.rdf.protocols.Quad{:s "http://john-doe.com/id/johnd", :p "http://www.w3.org/2000/01/rdf-schema#label", :o #<rdf$s$reify__5355 John Doe>, :c "http://mygraph.com/graph/test"})
#+END_SRC

* Error Handling at Graph Construction

The biggest remaining challenge with the current RDF DSL is handling
error conditions, as I really don't want users to ever have to write
if statements.  For example some columns such as postcodes are
optional e.g.

#+BEGIN_SRC clojure
 [facility-uri
  [vcard:hasAddress [[rdf:a vcard:Address]
                     [vcard:street-address address]
                     [vcard:postal-code postcode-uri]]]]
#+END_SRC

What happens here (if you're not careful) is that you end up with an
error when you load the data because you can't have a triple with a
blank object.

So what you want to happen is to construct the facility with the
address vcard, but to leave out the whole postcode triple.  But you
don't want to always do this, as you want the user to be alerted to
errors; you really need them to indicate that it's ok for this field
to be optional.

One idea I've had that would cover this case is to use Clojure's
hash-map syntax to indicate optionality e.g. instead of the above you
would write:

#+BEGIN_SRC clojure
[facility-uri
 [vcard:hasAddress [[rdf:a vcard:Address]
                    [vcard:street-address address]
                    {vcard:postal-code postcode-uri}]]]
#+END_SRC

Which would mean omit this triple if either =vcard:postal-code= or
=postcode-uri= were =nil=.  This would work quite nicely, and is
easily done, because the whole triple is being created.

However there is a more complex case we should consider too.  In the
attendances dataset there is a "city wide" token which can be found in
the street-address field.  Where this occurs, we need to bail out of
creating an address bnode.

The first bit is easily dealt with already, by specifying a pipeline
mapper like so:

#+BEGIN_SRC clojure
(let [street-address   (with-monad blank-m (m-chain [trim (mapper "city wide" "") rdfstr])])
#+END_SRC

This basically means the "city wide" string will be converted to a
blank error value, which can signal failure later, when we come to
create the graph.

The bigger problem is finding a convenient way to undo work we may
have already done.

Below is one proposed method of doing this.  We simply indicate (with
a hashmap) that the above layer vcard:hasAddress predicate/object pair
is optional:

#+BEGIN_SRC clojure
(graph "http://foobar.com/"
                    [facility-uri
                     [rdfs:label name]
                     {vcard:hasAddress [[rdf:a vcard:Address]
                                        [vcard:street-address address]
                                        {vcard:postal-code postcode-uri}]}]])
#+END_SRC

An optional clause should not commit any triples within it to the
stream until the whole clause has succeeded.  If any value contains an
error, the statement should fail up to enclosing {}.

Each of these rows of rdf-types is then passed to a user defined
graphify function.  graphify is actually a macro, that takes an
argument list, where each argument receives a single cell value, but
conceptually maps to a column in the final table.

This is probably good enough for most cases but you could imagine
other complexities for example, you might want to fail the whole tree,
or the tree to an arbitrary parent.  Also like with SPARQL, you may
need to allow optional, optionals (probably unavoidable).  I quite
like this idea because it shares some parity with SPARQL constructs.

Triple Sinks and Type Coercion

As mentioned earlier,
